{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original', data_home='../')\n",
    "mnist.target = np.array(pd.get_dummies(mnist.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "holdout_set_mask = np.array([i % 10 == 0 for i in range(len(mnist.data))])\n",
    "\n",
    "X = mnist.data[~holdout_set_mask]\n",
    "y = mnist.target[~holdout_set_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaseTrainingBatchGenerator(metaclass=ABCMeta):\n",
    "    \n",
    "    def __init__(self, X, y, n_batches):\n",
    "        np.random.shuffle(X)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_batches = n_batches\n",
    "        \n",
    "    @abstractmethod\n",
    "    def __iter__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaseLossFunction(metaclass=ABCMeta):\n",
    "    \n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def loss_function(y_true, y_predicted):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def derivative_of_loss_function(y_true, y_predicted):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaseActivationFunction(metaclass=ABCMeta):\n",
    "    \n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def activation_function(linear_combination):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def derivative_of_activation_function(linear_combination):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaseOptimizationAlgorithm(metaclass=ABCMeta):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def run(self):\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HoldoutData:\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainingBatch:\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MeanSquaredError(BaseLossFunction):\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss_function(y_true, y_predicted):\n",
    "        return .5*(y_true - y_predicted)**2\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_of_loss_function(y_true, y_predicted):\n",
    "        return y_predicted - y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MiniBatchGenerator(BaseTrainingBatchGenerator):\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch_index in np.array_split(range(len(self.X)), self.n_batches):\n",
    "            yield TrainingBatch(X=self.X[batch_index], y=self.y[batch_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FullBatchGenerator(BaseTrainingBatchGenerator):\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SigmoidActivationFunction(BaseActivationFunction):\n",
    "    \n",
    "    @staticmethod\n",
    "    def activation_function(linear_combination):\n",
    "        return 1/(1 + np.exp(-linear_combination))\n",
    "\n",
    "    @classmethod\n",
    "    def derivative_of_activation_function(cls, linear_combination):\n",
    "        return cls.activation_function(linear_combination) * (1 - cls.activation_function(linear_combination))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GradientDescent(BaseOptimizationAlgorithm):\n",
    "    \n",
    "    def __init__(self, weight_matrices, bias_vectors, linear_combinations, activations, y, \n",
    "                 activation_function_class, loss_function_class, learning_rate):\n",
    "        self.weight_matrices=weight_matrices\n",
    "        self.bias_vectors=bias_vectors\n",
    "        self.linear_combinations = linear_combinations\n",
    "        self.activations=activations\n",
    "        self.y=y\n",
    "        self.activation_function_class = activation_function_class\n",
    "        self.loss_function_class=loss_function_class\n",
    "        self.learning_rate=learning_rate\n",
    "        self.batch_size = len(self.y)\n",
    "        \n",
    "    def run(self):\n",
    "        delta_matrices = self._compute_delta_matrices()\n",
    "        updated_weight_matrices = self._update_weight_matrices(delta_matrices)\n",
    "        updated_bias_vectors = self._update_bias_vectors(delta_matrices)\n",
    "        return updated_weight_matrices, updated_bias_vectors\n",
    "    \n",
    "    def _compute_delta_matrices(self):\n",
    "        output_layer_delta_matrix = self._compute_output_layer_delta_matrix()\n",
    "        delta_matrices = deque([output_layer_delta_matrix])\n",
    "        for linear_combination, weight_matrix in zip(reversed(self.linear_combinations[:-1]), reversed(self.weight_matrices)):\n",
    "            delta_matrix = np.dot(delta_matrices[-1], weight_matrix.T) * \\\n",
    "                self.activation_function_class.derivative_of_activation_function(linear_combination)\n",
    "            delta_matrices.appendleft(delta_matrix)\n",
    "        return delta_matrices\n",
    "    \n",
    "    def _compute_output_layer_delta_matrix(self):\n",
    "        return self.loss_function_class.derivative_of_loss_function(y_true=self.y, y_predicted=self.activations[-1]) * \\\n",
    "            self.activation_function_class.derivative_of_activation_function(self.linear_combinations[-1])\n",
    "        \n",
    "    def _update_weight_matrices(self, delta_matrices):\n",
    "        weight_gradient_matrices = self._compute_weight_gradient_matrices(delta_matrices)\n",
    "        return [weight_matrix + (-self.learning_rate*weight_gradient_matrix/self.batch_size) for weight_matrix, \\\n",
    "            weight_gradient_matrix in zip(self.weight_matrices, weight_gradient_matrices)]\n",
    "    \n",
    "    def _compute_weight_gradient_matrices(self, delta_matrices):\n",
    "        weight_gradient_matrices = deque()\n",
    "        for activation_matrix, delta_matrix in zip(reversed(self.activations[:-1]), reversed(delta_matrices)):\n",
    "            weight_gradient_matrices.appendleft(np.dot(activation_matrix.T, delta_matrix))\n",
    "        return weight_gradient_matrices\n",
    "    \n",
    "    def _update_bias_vectors(self, delta_matrices):\n",
    "        bias_gradient_vectors = self._compute_bias_gradient_vectors(delta_matrices)\n",
    "        return [bias_vector + (-self.learning_rate*bias_gradient_vector/self.batch_size) for bias_vector, \\\n",
    "            bias_gradient_vector in zip(self.bias_vectors, bias_gradient_vectors)]\n",
    "    \n",
    "    def _compute_bias_gradient_vectors(self, delta_matrices):\n",
    "        return [delta_matrix.sum(axis=0) for delta_matrix in delta_matrices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VanillaNeuralNet:\n",
    "    \n",
    "    def __init__(self, layer_sizes, training_batch_generator_class, loss_function_class, activation_function_class, \n",
    "                 optimization_algorithm_class, learning_rate, n_epochs, n_batches_per_epoch, holdout_data):\n",
    "        self.weight_matrices = self._initialize_weight_matrices(layer_sizes)\n",
    "        self.bias_vectors = self._initialize_bias_vectors(layer_sizes)\n",
    "        self.training_batch_generator_class = training_batch_generator_class\n",
    "        self.loss_function_class = loss_function_class\n",
    "        self.activation_function_class = activation_function_class\n",
    "        self.optimization_algorithm_class = optimization_algorithm_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.n_batches_per_epoch = n_batches_per_epoch\n",
    "        self.holdout_data = holdout_data\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for epoch in range(self.n_epochs):\n",
    "            print('Epoch {}'.format(epoch))\n",
    "            training_batch_generator = self.training_batch_generator_class(X=X, y=y, n_batches=self.n_batches_per_epoch)\n",
    "            \n",
    "            for training_batch in training_batch_generator:\n",
    "                self._update_weights_and_biases(training_batch)\n",
    "            self._validate_on_holdout_set()\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_combination_matrices, activations = self._feed_forward(X)\n",
    "        return activations[-1]\n",
    "    \n",
    "    def _update_weights_and_biases(self, training_batch):\n",
    "        linear_combinations, activations = self._feed_forward(training_batch.X)\n",
    "        self._back_propagate(linear_combinations=linear_combinations, activations=activations, y=training_batch.y)\n",
    "\n",
    "    def _feed_forward(self, X):\n",
    "        activation_matrices = [X]\n",
    "        linear_combination_matrices = []\n",
    "        for weight_matrix, bias_vector in zip(self.weight_matrices, self.bias_vectors):\n",
    "            linear_combination = np.dot(activation_matrices[-1], weight_matrix) + bias_vector\n",
    "            linear_combination_matrices.append(linear_combination)\n",
    "            activation_matrix = self.activation_function_class.activation_function(linear_combination)\n",
    "            activation_matrices.append(activation_matrix)\n",
    "        return linear_combination_matrices, activation_matrices\n",
    "        \n",
    "    def _back_propagate(self, linear_combinations, activations, y):\n",
    "        self.weight_matrices, self.bias_vectors = self.optimization_algorithm_class(\n",
    "            weight_matrices=self.weight_matrices,\n",
    "            bias_vectors=self.bias_vectors,\n",
    "            linear_combinations=linear_combinations,\n",
    "            activations=activations,\n",
    "            y=y,\n",
    "            activation_function_class=self.activation_function_class,\n",
    "            loss_function_class=self.loss_function_class,\n",
    "            learning_rate=self.learning_rate\n",
    "        ).run()\n",
    "        \n",
    "    def _validate_on_holdout_set(self):\n",
    "        holdout_predictions = self.predict(self.holdout_data.X)\n",
    "        holdout_error = self.loss_function_class.loss_function(\n",
    "            y_true=self.holdout_data.y,\n",
    "            y_predicted=holdout_predictions\n",
    "        ).mean()\n",
    "        print('Holdout error: {}'.format(np.round(holdout_error, 5)))\n",
    "        \n",
    "    @staticmethod\n",
    "    def _initialize_weight_matrices(layer_sizes):\n",
    "        return [np.random.randn(layer_size, next_layer_size) for layer_size, next_layer_size \\\n",
    "                in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _initialize_bias_vectors(layer_sizes):\n",
    "        return [np.random.randn(layer_size) for layer_size in layer_sizes[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_LAYER_SIZE = 100\n",
    "LEARNING_RATE = .05\n",
    "N_EPOCHS = 100\n",
    "N_BATCHES_PER_EPOCH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LAYER_SIZES = [mnist.data.shape[1], HIDDEN_LAYER_SIZE, mnist.target.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vanilla_neural_net = VanillaNeuralNet(\n",
    "    layer_sizes=LAYER_SIZES,\n",
    "    training_batch_generator_class=MiniBatchGenerator,\n",
    "    loss_function_class=MeanSquaredError,\n",
    "    activation_function_class=SigmoidActivationFunction,\n",
    "    optimization_algorithm_class=GradientDescent,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    n_batches_per_epoch=N_BATCHES_PER_EPOCH,\n",
    "    holdout_data=HoldoutData(X=mnist.data[holdout_set_mask], y=mnist.target[holdout_set_mask])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Holdout error: 0.17125\n",
      "Epoch 1\n",
      "Holdout error: 0.14791\n",
      "Epoch 2\n",
      "Holdout error: 0.12846\n",
      "Epoch 3\n",
      "Holdout error: 0.11248\n",
      "Epoch 4\n",
      "Holdout error: 0.10003\n",
      "Epoch 5\n",
      "Holdout error: 0.09062\n",
      "Epoch 6\n",
      "Holdout error: 0.08319\n",
      "Epoch 7\n",
      "Holdout error: 0.0774\n",
      "Epoch 8\n",
      "Holdout error: 0.07299\n",
      "Epoch 9\n",
      "Holdout error: 0.06954\n",
      "Epoch 10\n",
      "Holdout error: 0.06682\n",
      "Epoch 11\n",
      "Holdout error: 0.06467\n",
      "Epoch 12\n",
      "Holdout error: 0.06298\n",
      "Epoch 13\n",
      "Holdout error: 0.06152\n",
      "Epoch 14\n",
      "Holdout error: 0.06033\n",
      "Epoch 15\n",
      "Holdout error: 0.05935\n",
      "Epoch 16\n",
      "Holdout error: 0.05854\n",
      "Epoch 17\n",
      "Holdout error: 0.05783\n",
      "Epoch 18\n",
      "Holdout error: 0.05721\n",
      "Epoch 19\n",
      "Holdout error: 0.05667\n",
      "Epoch 20\n",
      "Holdout error: 0.05618\n",
      "Epoch 21\n",
      "Holdout error: 0.05575\n",
      "Epoch 22\n",
      "Holdout error: 0.05538\n",
      "Epoch 23\n",
      "Holdout error: 0.05507\n",
      "Epoch 24\n",
      "Holdout error: 0.05477\n",
      "Epoch 25\n",
      "Holdout error: 0.05452\n",
      "Epoch 26\n",
      "Holdout error: 0.05428\n",
      "Epoch 27\n",
      "Holdout error: 0.05409\n",
      "Epoch 28\n",
      "Holdout error: 0.0539\n",
      "Epoch 29\n",
      "Holdout error: 0.05373\n",
      "Epoch 30\n",
      "Holdout error: 0.05358\n",
      "Epoch 31\n",
      "Holdout error: 0.05343\n",
      "Epoch 32\n",
      "Holdout error: 0.05328\n",
      "Epoch 33\n",
      "Holdout error: 0.05316\n",
      "Epoch 34\n",
      "Holdout error: 0.05304\n",
      "Epoch 35\n",
      "Holdout error: 0.05293\n",
      "Epoch 36\n",
      "Holdout error: 0.05282\n",
      "Epoch 37\n",
      "Holdout error: 0.05273\n",
      "Epoch 38\n",
      "Holdout error: 0.05264\n",
      "Epoch 39\n",
      "Holdout error: 0.05256\n",
      "Epoch 40\n",
      "Holdout error: 0.05248\n",
      "Epoch 41\n",
      "Holdout error: 0.0524\n",
      "Epoch 42\n",
      "Holdout error: 0.05233\n",
      "Epoch 43\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-f20e256f3540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvanilla_neural_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-f1ed09442754>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtraining_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_batch_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_weights_and_biases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_on_holdout_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-f1ed09442754>\u001b[0m in \u001b[0;36m_update_weights_and_biases\u001b[0;34m(self, training_batch)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_weights_and_biases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mlinear_combinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_back_propagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_combinations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinear_combinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_feed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-f1ed09442754>\u001b[0m in \u001b[0;36m_back_propagate\u001b[0;34m(self, linear_combinations, activations, y)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mactivation_function_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mloss_function_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         ).run()\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-f126f7f66d68>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdelta_matrices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_delta_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mupdated_weight_matrices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_weight_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_matrices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mupdated_bias_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_bias_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_matrices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-f126f7f66d68>\u001b[0m in \u001b[0;36m_compute_delta_matrices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mdelta_matrices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_layer_delta_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlinear_combination\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_matrix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_combinations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_matrices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mdelta_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_matrices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderivative_of_activation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_combination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mdelta_matrices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappendleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdelta_matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-0608e1c21bfc>\u001b[0m in \u001b[0;36mderivative_of_activation_function\u001b[0;34m(cls, linear_combination)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mderivative_of_activation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_combination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_combination\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_combination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-0608e1c21bfc>\u001b[0m in \u001b[0;36mactivation_function\u001b[0;34m(linear_combination)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_combination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlinear_combination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vanilla_neural_net.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
