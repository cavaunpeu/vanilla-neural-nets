{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/willwolf/Documents/neural-nets/vanilla-neural-nets\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets.samples_generator import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from vanilla_neural_nets.neural_network.training_batch_generator import MiniBatchGenerator\n",
    "from vanilla_neural_nets.neural_network.optimization_algorithm import GradientDescent\n",
    "from vanilla_neural_nets.neural_network.activation_function import ReLUActivationFunction, LinearActivationFunction\n",
    "from vanilla_neural_nets.neural_network.loss_function import MeanSquaredError\n",
    "from vanilla_neural_nets.neural_network.network import VanillaNeuralNetwork\n",
    "from vanilla_neural_nets.neural_network.data_object import HoldoutData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a random regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_full, y_full = make_regression(n_samples=100000, n_features=100, noise=.1, coef=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_full = y_full.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate and scale train and holdout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holdout_set_mask = np.array([i % 7 == 0 for i in range(len(X_full))])\n",
    "np.random.shuffle(holdout_set_mask)\n",
    "\n",
    "X = X_full[~holdout_set_mask].astype(float)\n",
    "y = y_full[~holdout_set_mask].astype(float)\n",
    "X_holdout = X_full[holdout_set_mask].astype(float)\n",
    "y_holdout = y_full[holdout_set_mask].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = MinMaxScaler().fit_transform(X)\n",
    "X_holdout = MinMaxScaler().fit_transform(X_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_LAYER_SIZE = 50\n",
    "LEARNING_RATE = .05\n",
    "N_EPOCHS = 10\n",
    "TRAINING_BATCH_SIZE = 10\n",
    "RANDOM_STATE = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LAYER_SIZES = [X.shape[1], HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with mean squared error, ReLU activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vanilla_neural_net = VanillaNeuralNetwork(\n",
    "    layer_sizes=LAYER_SIZES,\n",
    "    training_batch_generator_class=MiniBatchGenerator,\n",
    "    loss_function_class=MeanSquaredError,\n",
    "    activation_function_class=ReLUActivationFunction,\n",
    "    output_layer_activation_function_class=LinearActivationFunction,\n",
    "    optimization_algorithm_class=GradientDescent,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    training_batch_size=TRAINING_BATCH_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    holdout_data=HoldoutData(X=X_holdout, y=y_holdout)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "> \u001b[0;32m/Users/willwolf/Documents/neural-nets/vanilla-neural-nets/neural_network/optimization_algorithm.py\u001b[0m(47)\u001b[0;36m_feed_forward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     45 \u001b[0;31m                \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# breakpoint eef0b94a //\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     46 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 47 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_combination_matrices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_combination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     48 \u001b[0;31m            \u001b[0mactivation_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_function_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_combination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     49 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_matrices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u0001\u0002ipdb> \u0001\u0002self.activation_matrices[-1]\n",
      "array([[ 0.65811131,  0.74006436,  0.40851725,  0.38866543,  0.47580035,\n",
      "         0.62827851,  0.62643857,  0.49619369,  0.35020139,  0.70527812,\n",
      "         0.35242451,  0.61243319,  0.43203149,  0.55897377,  0.70447686,\n",
      "         0.34689154,  0.4988244 ,  0.44943423,  0.51487156,  0.61331357,\n",
      "         0.53455136,  0.49877361,  0.34530661,  0.26688126,  0.48028189,\n",
      "         0.36755907,  0.48857207,  0.56548055,  0.42146169,  0.61979646,\n",
      "         0.38564737,  0.41031984,  0.58055912,  0.56200022,  0.38804696,\n",
      "         0.52025072,  0.52828951,  0.42031576,  0.44930043,  0.67280086,\n",
      "         0.61068623,  0.48852343,  0.38194797,  0.50824679,  0.36651956,\n",
      "         0.58638512,  0.45072861,  0.52980103,  0.58606826,  0.43884779,\n",
      "         0.65176281,  0.53809688,  0.3960734 ,  0.4632426 ,  0.62232826,\n",
      "         0.37650266,  0.52922126,  0.64523581,  0.48105116,  0.35090235,\n",
      "         0.76824125,  0.39728426,  0.6103418 ,  0.38850215,  0.5689918 ,\n",
      "         0.33431134,  0.54615705,  0.49032608,  0.70129869,  0.51749977,\n",
      "         0.36841747,  0.47631573,  0.28908711,  0.5508965 ,  0.47921844,\n",
      "         0.43002108,  0.48963582,  0.47995231,  0.56420648,  0.63587983,\n",
      "         0.47376366,  0.58513116,  0.31631801,  0.5327731 ,  0.51900961,\n",
      "         0.49467861,  0.59192031,  0.50822627,  0.35845072,  0.502465  ,\n",
      "         0.46593583,  0.57688769,  0.51412064,  0.43955643,  0.39174612,\n",
      "         0.47425727,  0.57224433,  0.52183188,  0.43260284,  0.44432661],\n",
      "       [ 0.42746435,  0.42374181,  0.36909019,  0.60728833,  0.44680778,\n",
      "         0.63611346,  0.38977016,  0.40126894,  0.56097388,  0.49573207,\n",
      "         0.31328311,  0.41723259,  0.47526292,  0.47497015,  0.32976954,\n",
      "         0.51342219,  0.73503212,  0.49748301,  0.53370791,  0.68858929,\n",
      "         0.58221623,  0.47653662,  0.35692704,  0.64808079,  0.1044924 ,\n",
      "         0.35674281,  0.41317871,  0.36788999,  0.57308011,  0.50839955,\n",
      "         0.53000946,  0.45445791,  0.73750941,  0.70074308,  0.51069415,\n",
      "         0.39195422,  0.5336272 ,  0.44152368,  0.48622385,  0.24051545,\n",
      "         0.42590558,  0.53119773,  0.54844166,  0.48009439,  0.51294391,\n",
      "         0.62923473,  0.48269493,  0.52472868,  0.47927331,  0.44752233,\n",
      "         0.35568397,  0.6630177 ,  0.34710442,  0.36142667,  0.60607267,\n",
      "         0.37483704,  0.42472454,  0.2919408 ,  0.64438963,  0.6722965 ,\n",
      "         0.33685448,  0.43000047,  0.48051278,  0.46008753,  0.42317643,\n",
      "         0.30896177,  0.56185711,  0.37259044,  0.38009329,  0.51483242,\n",
      "         0.54939799,  0.59985029,  0.62742778,  0.42554271,  0.69592775,\n",
      "         0.64655593,  0.35850971,  0.34171242,  0.63671334,  0.48969469,\n",
      "         0.36844361,  0.27191971,  0.57332897,  0.74417539,  0.49640789,\n",
      "         0.4368848 ,  0.55759322,  0.41279905,  0.59928803,  0.36273021,\n",
      "         0.59995456,  0.36914783,  0.50337438,  0.37983181,  0.61236651,\n",
      "         0.48409898,  0.5106137 ,  0.4363598 ,  0.44085269,  0.39287349],\n",
      "       [ 0.58786287,  0.61650297,  0.50769185,  0.37034233,  0.58199917,\n",
      "         0.35540172,  0.62260824,  0.54054324,  0.41122804,  0.3776639 ,\n",
      "         0.25100254,  0.56800235,  0.50649055,  0.76459049,  0.72674717,\n",
      "         0.4281368 ,  0.54556482,  0.53005522,  0.79252198,  0.48081304,\n",
      "         0.57077558,  0.32629008,  0.56616445,  0.38260207,  0.44146021,\n",
      "         0.43952638,  0.24279017,  0.60518427,  0.48356461,  0.65267501,\n",
      "         0.33535779,  0.66792229,  0.52031811,  0.47317114,  0.33276193,\n",
      "         0.53555164,  0.51184945,  0.5771291 ,  0.41135426,  0.58529918,\n",
      "         0.64308851,  0.46245238,  0.65233124,  0.48288072,  0.27531107,\n",
      "         0.44911334,  0.50242683,  0.38795985,  0.44745108,  0.52487011,\n",
      "         0.47192106,  0.6234941 ,  0.39958298,  0.32754693,  0.75607807,\n",
      "         0.42997316,  0.47062366,  0.41928132,  0.27778686,  0.65624112,\n",
      "         0.48915872,  0.43977746,  0.38544604,  0.44146131,  0.67051357,\n",
      "         0.33993894,  0.58967256,  0.61173124,  0.60253959,  0.46090094,\n",
      "         0.49869287,  0.61390064,  0.45373538,  0.64228784,  0.49169692,\n",
      "         0.52844881,  0.5005854 ,  0.49874261,  0.56835198,  0.72335108,\n",
      "         0.68414792,  0.64442589,  0.46117824,  0.59452924,  0.38985318,\n",
      "         0.63776513,  0.51049863,  0.50328671,  0.57144247,  0.66970901,\n",
      "         0.45759015,  0.36575617,  0.50879792,  0.46058906,  0.52726588,\n",
      "         0.49164022,  0.60722471,  0.44809591,  0.36970601,  0.36447984],\n",
      "       [ 0.52268937,  0.51771722,  0.39316005,  0.53742175,  0.34652381,\n",
      "         0.41672078,  0.53550636,  0.53579857,  0.3517569 ,  0.34156184,\n",
      "         0.67050099,  0.44626889,  0.65561669,  0.58715305,  0.62117263,\n",
      "         0.47410153,  0.39259956,  0.47152279,  0.53207473,  0.47579853,\n",
      "         0.38070179,  0.35702147,  0.31562699,  0.56428733,  0.32559439,\n",
      "         0.61545879,  0.68186758,  0.29146554,  0.64734052,  0.52678692,\n",
      "         0.56051539,  0.68673418,  0.5907411 ,  0.6149061 ,  0.64516353,\n",
      "         0.46884133,  0.57384884,  0.5306168 ,  0.65945774,  0.46900914,\n",
      "         0.41834968,  0.30423487,  0.50053689,  0.58513481,  0.50132322,\n",
      "         0.30660081,  0.47398295,  0.47765031,  0.70095804,  0.7401887 ,\n",
      "         0.67788225,  0.61763428,  0.63267006,  0.66201177,  0.60703464,\n",
      "         0.45640507,  0.66971247,  0.45583057,  0.6765844 ,  0.45807577,\n",
      "         0.36113993,  0.64077671,  0.59626042,  0.44386164,  0.70588324,\n",
      "         0.44203431,  0.47324148,  0.53499971,  0.49302897,  0.45472876,\n",
      "         0.59261241,  0.43641578,  0.43080621,  0.63340033,  0.43669117,\n",
      "         0.70384858,  0.52839376,  0.34988366,  0.58678341,  0.65174141,\n",
      "         0.36917114,  0.52033289,  0.2825373 ,  0.38504686,  0.383712  ,\n",
      "         0.46587581,  0.48008374,  0.48583182,  0.74388634,  0.61561391,\n",
      "         0.7726431 ,  0.58678104,  0.55653549,  0.5272866 ,  0.26223955,\n",
      "         0.36018224,  0.39408207,  0.60393657,  0.5151965 ,  0.63671626],\n",
      "       [ 0.504887  ,  0.63364519,  0.45610278,  0.40473569,  0.4578163 ,\n",
      "         0.59865837,  0.40426046,  0.41979562,  0.54875249,  0.32625973,\n",
      "         0.46212699,  0.24569558,  0.57863923,  0.54822909,  0.4391181 ,\n",
      "         0.54740476,  0.44223591,  0.53669799,  0.42271799,  0.51056407,\n",
      "         0.68937423,  0.36256381,  0.5703836 ,  0.47518189,  0.35534006,\n",
      "         0.30861033,  0.57733283,  0.27828959,  0.42194589,  0.45109675,\n",
      "         0.36499177,  0.29247987,  0.32275705,  0.39157174,  0.49395046,\n",
      "         0.38688838,  0.59345778,  0.39873838,  0.5669496 ,  0.59239276,\n",
      "         0.47370409,  0.33736197,  0.31292208,  0.39830839,  0.486502  ,\n",
      "         0.51565889,  0.58369883,  0.44671323,  0.46784359,  0.33786733,\n",
      "         0.43523412,  0.41200247,  0.4443824 ,  0.38773224,  0.58661357,\n",
      "         0.53803338,  0.46811638,  0.49759309,  0.65378514,  0.588543  ,\n",
      "         0.58371715,  0.4505444 ,  0.47820672,  0.3991653 ,  0.35269906,\n",
      "         0.49771913,  0.31881978,  0.39431234,  0.45101374,  0.23016251,\n",
      "         0.45020796,  0.45615155,  0.30972399,  0.50144041,  0.5145394 ,\n",
      "         0.60142305,  0.51969542,  0.26687259,  0.83009001,  0.25072912,\n",
      "         0.45338658,  0.47071237,  0.4430175 ,  0.39298891,  0.60253095,\n",
      "         0.36111653,  0.33193753,  0.33975707,  0.5376952 ,  0.34446188,\n",
      "         0.36846919,  0.28614158,  0.45161478,  0.60975882,  0.35300348,\n",
      "         0.38115502,  0.57082535,  0.41659763,  0.72089538,  0.50474018],\n",
      "       [ 0.48659518,  0.48308492,  0.3830391 ,  0.42498279,  0.51439526,\n",
      "         0.47772229,  0.5878341 ,  0.40035919,  0.46851353,  0.54363751,\n",
      "         0.65102962,  0.54575862,  0.6286296 ,  0.4925023 ,  0.52455716,\n",
      "         0.47571793,  0.45436834,  0.37489651,  0.67240263,  0.47063293,\n",
      "         0.57479816,  0.48952017,  0.54043951,  0.45476623,  0.45675553,\n",
      "         0.47428124,  0.27307443,  0.6049961 ,  0.64597055,  0.43765889,\n",
      "         0.24011485,  0.59774239,  0.45288431,  0.59758661,  0.60830297,\n",
      "         0.45886238,  0.36117335,  0.52425715,  0.54946767,  0.53800016,\n",
      "         0.71562687,  0.41151193,  0.7828988 ,  0.47262733,  0.45169452,\n",
      "         0.80214038,  0.46922775,  0.26126816,  0.33244135,  0.61182994,\n",
      "         0.41746145,  0.42908553,  0.32611265,  0.50911611,  0.60379313,\n",
      "         0.4116025 ,  0.48942137,  0.3839457 ,  0.36032288,  0.6190327 ,\n",
      "         0.45166682,  0.49794368,  0.48177109,  0.54161415,  0.66373335,\n",
      "         0.51255677,  0.46502886,  0.59084396,  0.62594041,  0.48723111,\n",
      "         0.52096622,  0.30388194,  0.58323836,  0.54937051,  0.51715357,\n",
      "         0.38212395,  0.5276775 ,  0.59485566,  0.64229954,  0.46712843,\n",
      "         0.56021566,  0.50397711,  0.47851556,  0.11602165,  0.55232873,\n",
      "         0.85968583,  0.55854818,  0.42947744,  0.53781142,  0.47014237,\n",
      "         0.37332286,  0.60067281,  0.63396668,  0.47092366,  0.70057469,\n",
      "         0.5830097 ,  0.57832084,  0.65143397,  0.56207971,  0.47714416],\n",
      "       [ 0.47864908,  0.52842454,  0.34378987,  0.57908467,  0.38730881,\n",
      "         0.41502012,  0.48116343,  0.47591883,  0.35026397,  0.56210041,\n",
      "         0.51024552,  0.43704689,  0.50559769,  0.43304461,  0.51071313,\n",
      "         0.54858661,  0.60712659,  0.59063913,  0.55032638,  0.51952027,\n",
      "         0.5894875 ,  0.63032926,  0.58813396,  0.57372688,  0.54107667,\n",
      "         0.46678167,  0.47744686,  0.63135046,  0.34062566,  0.4984655 ,\n",
      "         0.5533134 ,  0.49733342,  0.41439767,  0.5875711 ,  0.46300354,\n",
      "         0.34548108,  0.6828121 ,  0.33981065,  0.41718677,  0.36675294,\n",
      "         0.37919592,  0.47763818,  0.46109594,  0.41280719,  0.31198114,\n",
      "         0.33079473,  0.50853014,  0.44602701,  0.40928701,  0.45116625,\n",
      "         0.6081954 ,  0.49705604,  0.48199081,  0.49456667,  0.57198035,\n",
      "         0.38716792,  0.6002323 ,  0.55077441,  0.60482928,  0.54040421,\n",
      "         0.60507772,  0.34849619,  0.40326455,  0.35541893,  0.5290483 ,\n",
      "         0.52097129,  0.5231581 ,  0.55337745,  0.45300729,  0.54371368,\n",
      "         0.59074908,  0.42920941,  0.65998722,  0.36955823,  0.53840679,\n",
      "         0.66986426,  0.60464983,  0.45704929,  0.52195615,  0.4451592 ,\n",
      "         0.40145703,  0.24658657,  0.31111163,  0.45752176,  0.42621248,\n",
      "         0.44711257,  0.48842991,  0.46301765,  0.53438565,  0.55526636,\n",
      "         0.48735817,  0.60171082,  0.70135429,  0.75838371,  0.43830058,\n",
      "         0.34764698,  0.49645708,  0.35219965,  0.34886995,  0.4807488 ],\n",
      "       [ 0.58374532,  0.49201759,  0.57559519,  0.39672976,  0.55264589,\n",
      "         0.42547564,  0.40396371,  0.41320308,  0.48811791,  0.4744826 ,\n",
      "         0.62101072,  0.34693225,  0.42874384,  0.57337265,  0.57457576,\n",
      "         0.48268084,  0.64852983,  0.57787524,  0.63718575,  0.6542649 ,\n",
      "         0.38601546,  0.57815993,  0.47585468,  0.64571109,  0.46106565,\n",
      "         0.59024223,  0.43270513,  0.67460872,  0.58855113,  0.46582491,\n",
      "         0.5867636 ,  0.90628471,  0.52965092,  0.45388349,  0.57436305,\n",
      "         0.54831553,  0.53332643,  0.53252145,  0.53036912,  0.30918893,\n",
      "         0.38181838,  0.4723508 ,  0.68639916,  0.56674339,  0.56281906,\n",
      "         0.26031593,  0.62777066,  0.65832897,  0.56826096,  0.37291228,\n",
      "         0.31021082,  0.31955156,  0.48551175,  0.6991188 ,  0.67521672,\n",
      "         0.31469653,  0.41544532,  0.78587222,  0.73636911,  0.63338131,\n",
      "         0.31227453,  0.48996753,  0.2506325 ,  0.66061904,  0.47950506,\n",
      "         0.46763731,  0.48662695,  0.48434221,  0.55751565,  0.66276987,\n",
      "         0.50820421,  0.50388568,  0.80192948,  0.3397918 ,  0.533188  ,\n",
      "         0.51483799,  0.78314396,  0.36180031,  0.42253096,  0.53768507,\n",
      "         0.63000187,  0.6116094 ,  0.27508187,  0.42357027,  0.58882987,\n",
      "         0.54013743,  0.56445082,  0.48675848,  0.46315733,  0.55320587,\n",
      "         0.44876571,  0.44012893,  0.57324986,  0.69105076,  0.41419137,\n",
      "         0.47270607,  0.38361688,  0.69010322,  0.32249737,  0.50515461],\n",
      "       [ 0.36920954,  0.60960547,  0.22932608,  0.44306616,  0.58539763,\n",
      "         0.6074786 ,  0.3883438 ,  0.67098193,  0.30911674,  0.56658645,\n",
      "         0.62634031,  0.84828459,  0.33080593,  0.35088528,  0.52549349,\n",
      "         0.48086734,  0.69552354,  0.48129183,  0.52431726,  0.56401679,\n",
      "         0.38446161,  0.19132152,  0.48356707,  0.66245613,  0.59600899,\n",
      "         0.41016075,  0.47586262,  0.51297438,  0.47897249,  0.49240595,\n",
      "         0.38693036,  0.53261913,  0.5442834 ,  0.52890847,  0.42072043,\n",
      "         0.54301098,  0.62866958,  0.39926662,  0.6446088 ,  0.50445814,\n",
      "         0.56831946,  0.36264476,  0.58379534,  0.68458408,  0.66656138,\n",
      "         0.54712283,  0.35910318,  0.37798645,  0.81598846,  0.61970558,\n",
      "         0.51370666,  0.71277696,  0.64243338,  0.30303375,  0.73013182,\n",
      "         0.526001  ,  0.38345072,  0.63626806,  0.61039017,  0.44207669,\n",
      "         0.57765926,  0.44760026,  0.52535084,  0.67731492,  0.47676716,\n",
      "         0.7317423 ,  0.48106172,  0.57650726,  0.37097403,  0.50155029,\n",
      "         0.40136218,  0.53682018,  0.32397607,  0.32988835,  0.52592966,\n",
      "         0.26099449,  0.41534769,  0.50090526,  0.35194428,  0.36975965,\n",
      "         0.58968664,  0.33641778,  0.57515013,  0.46399852,  0.72886202,\n",
      "         0.45044774,  0.64416502,  0.48885115,  0.42616899,  0.31521691,\n",
      "         0.68774032,  0.50270499,  0.33464724,  0.40313287,  0.3856586 ,\n",
      "         0.52881074,  0.47702796,  0.44932838,  0.41460162,  0.54943403],\n",
      "       [ 0.62380781,  0.4483976 ,  0.49194963,  0.48494133,  0.43703237,\n",
      "         0.47575203,  0.65475891,  0.6639317 ,  0.47722569,  0.51514224,\n",
      "         0.51498694,  0.56816001,  0.44658342,  0.74885753,  0.78568624,\n",
      "         0.56711776,  0.6570972 ,  0.64433357,  0.5768453 ,  0.63687234,\n",
      "         0.46813998,  0.53714928,  0.51053119,  0.33470813,  0.52118348,\n",
      "         0.35814744,  0.47408449,  0.49808449,  0.3562358 ,  0.42174141,\n",
      "         0.33942955,  0.12391911,  0.32004935,  0.40064106,  0.7010415 ,\n",
      "         0.32566257,  0.45113761,  0.502435  ,  0.72050623,  0.56525656,\n",
      "         0.52511574,  0.67300053,  0.5094425 ,  0.59883301,  0.39435973,\n",
      "         0.32415222,  0.6345645 ,  0.33537692,  0.46553741,  0.51220265,\n",
      "         0.41565997,  0.4234359 ,  0.54592758,  0.42574874,  0.4770714 ,\n",
      "         0.60770343,  0.58530089,  0.51739256,  0.47646105,  0.42605582,\n",
      "         0.45076303,  0.37915971,  0.73930307,  0.56816632,  0.6635461 ,\n",
      "         0.58292949,  0.48907138,  0.30642327,  0.5400983 ,  0.35603472,\n",
      "         0.55939365,  0.48374521,  0.73202359,  0.59226074,  0.3619989 ,\n",
      "         0.61061032,  0.55165059,  0.35365589,  0.41188206,  0.28592694,\n",
      "         0.56088347,  0.51610998,  0.41286374,  0.40565079,  0.40570377,\n",
      "         0.31937332,  0.7045292 ,  0.45121801,  0.49887993,  0.36205235,\n",
      "         0.4073932 ,  0.44515034,  0.72167522,  0.3796063 ,  0.45569664,\n",
      "         0.68013173,  0.64207111,  0.58612995,  0.50254944,  0.46805421]])\n",
      "\u0001\u0002ipdb> \u0001\u0002layer.weight_matrix\n",
      "array([[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       ..., \n",
      "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan, ...,  nan,  nan,  nan]])\n",
      "\u0001\u0002ipdb> \u0001\u0002exit\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f20e256f3540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvanilla_neural_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/willwolf/Documents/neural-nets/vanilla-neural-nets/neural_network/network.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtraining_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_batch_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_network_layers_with_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholdout_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mholdout_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_on_holdout_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {} | Accuracy: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mholdout_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/willwolf/Documents/neural-nets/vanilla-neural-nets/neural_network/network.py\u001b[0m in \u001b[0;36m_update_network_layers_with_training_batch\u001b[0;34m(self, training_batch)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0moutput_layer_activation_function_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer_activation_function_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         ).run()\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_on_holdout_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/willwolf/Documents/neural-nets/vanilla-neural-nets/neural_network/optimization_algorithm.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_delta_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_matrices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_updated_weight_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/willwolf/Documents/neural-nets/vanilla-neural-nets/neural_network/optimization_algorithm.py\u001b[0m in \u001b[0;36m_feed_forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mactivation_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_function_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_combination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_matrices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute_delta_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0moutput_layer_delta_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_output_layer_delta_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/willwolf/Documents/neural-nets/vanilla-neural-nets/neural_network/optimization_algorithm.py\u001b[0m in \u001b[0;36m_feed_forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mactivation_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_function_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_combination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_matrices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute_delta_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0moutput_layer_delta_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_output_layer_delta_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/willwolf/.pyenv/versions/3.4.3/lib/python3.4/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/willwolf/.pyenv/versions/3.4.3/lib/python3.4/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vanilla_neural_net.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
